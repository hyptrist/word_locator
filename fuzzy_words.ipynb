{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textdistance import Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(str1):\n",
    "    word=re.sub(r'^RT[\\s]+','',str1)\n",
    "    word=re.sub(r'\\$\\w*','',word)\n",
    "    word=re.sub(r'https?:\\/\\/.*[\\r\\n]*','',word)\n",
    "    word=re.sub(r'#','',word)\n",
    "    tokenizer=TweetTokenizer(preserve_case=False,strip_handles=True,reduce_len=True)\n",
    "    tweet_tokens=tokenizer.tokenize(str1)\n",
    "    word_clean=[]\n",
    "    for words in tweet_tokens:\n",
    "        if words not in string.punctuation:\n",
    "            word_clean.append(words)\n",
    "    return word_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['language',\n",
       " 'is',\n",
       " 'less',\n",
       " 'about',\n",
       " 'words',\n",
       " 'and',\n",
       " 'more',\n",
       " 'about',\n",
       " 'the',\n",
       " 'meaning',\n",
       " 'behind',\n",
       " 'them',\n",
       " 'if',\n",
       " 'you',\n",
       " 'spend',\n",
       " 'all',\n",
       " 'your',\n",
       " 'time',\n",
       " 'learning',\n",
       " 'vocabulary',\n",
       " 'and',\n",
       " 'grammar',\n",
       " 'you',\n",
       " 'will',\n",
       " 'never',\n",
       " 'be',\n",
       " 'able',\n",
       " 'to',\n",
       " 'fluently',\n",
       " 'speak',\n",
       " 'a',\n",
       " 'language',\n",
       " 'because',\n",
       " 'you',\n",
       " 'will',\n",
       " 'have',\n",
       " 'little',\n",
       " 'to',\n",
       " 'talk',\n",
       " 'about',\n",
       " 'these',\n",
       " 'short',\n",
       " 'stories',\n",
       " 'give',\n",
       " 'you',\n",
       " 'the',\n",
       " 'opportunity',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'big',\n",
       " 'ideas',\n",
       " 'in',\n",
       " 'context']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence='@apoorva Language is less about words and more about the meaning behind them. If you spend all your time learning vocabulary and grammar, you will never be able to fluently speak a language because you will have little to talk about. These short stories give you the opportunity to understand big ideas in context.'\n",
    "tok=clean_tweet(sentence)\n",
    "tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein(s2, s1)\n",
    "\n",
    "    # len(s1) >= len(s2)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1 # j+1 instead of j since previous_row and current_row are one character longer\n",
    "            deletions = current_row[j] + 1       # than s2\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    \n",
    "    return previous_row[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levenshtein('cherry', 'merry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language 8\n",
      "is 6\n",
      "less 5\n",
      "about 6\n",
      "words 5\n",
      "and 6\n",
      "more 5\n",
      "about 6\n",
      "the 4\n",
      "meaning 7\n",
      "behind 6\n",
      "them 4\n",
      "if 6\n",
      "you 6\n",
      "spend 5\n",
      "all 6\n",
      "your 5\n",
      "time 6\n",
      "learning 7\n",
      "vocabulary 7\n",
      "and 6\n",
      "grammar 7\n",
      "you 6\n",
      "will 6\n",
      "never 5\n",
      "be 5\n",
      "able 6\n",
      "to 6\n",
      "fluently 6\n",
      "speak 5\n",
      "a 6\n",
      "language 8\n",
      "because 7\n",
      "you 6\n",
      "will 6\n",
      "have 5\n",
      "little 6\n",
      "to 6\n",
      "talk 6\n",
      "about 6\n",
      "these 4\n",
      "short 4\n",
      "stories 6\n",
      "give 6\n",
      "you 6\n",
      "the 4\n",
      "opportunity 9\n",
      "to 6\n",
      "understand 8\n",
      "big 6\n",
      "ideas 5\n",
      "in 6\n",
      "context 6\n"
     ]
    }
   ],
   "source": [
    "for i in tok:\n",
    "    dist=levenshtein('cherry', i)\n",
    "    print(i,dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist1=53\n",
    "def min_dist(l1,dist1):\n",
    "    for i in l1:\n",
    "        dist=levenshtein('cherry', i)\n",
    "        if dist<=dist1:\n",
    "            dist1=dist\n",
    "        else:\n",
    "            continue\n",
    "    print([x for x in l1 if levenshtein('cherry', x)==dist1])\n",
    "    print(dist1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['them', 'the', 'these', 'short']\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "min_dist(tok,dist1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textdistance import levenshtein #will use this with pandas to reduce time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language\n",
      "is\n",
      "le\n",
      "about\n",
      "word\n",
      "and\n",
      "more\n",
      "about\n",
      "the\n",
      "meaning\n",
      "behind\n",
      "them\n",
      "if\n",
      "you\n",
      "spend\n",
      "all\n",
      "your\n",
      "time\n",
      "learning\n",
      "vocabulary\n",
      "and\n",
      "grammar\n",
      "you\n",
      "will\n",
      "never\n",
      "be\n",
      "able\n",
      "to\n",
      "fluently\n",
      "speak\n",
      "a\n",
      "language\n",
      "because\n",
      "you\n",
      "will\n",
      "have\n",
      "little\n",
      "to\n",
      "talk\n",
      "about\n",
      "these\n",
      "short\n",
      "story\n",
      "give\n",
      "you\n",
      "the\n",
      "opportunity\n",
      "to\n",
      "understand\n",
      "big\n",
      "idea\n",
      "in\n",
      "context\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "input_str=' '.join(tok)\n",
    "input_str=word_tokenize(input_str)\n",
    "for word in input_str:\n",
    "    print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Shreya\n",
      "[nltk_data]     Singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist1=100\n",
    "word='cherry'\n",
    "def min_dist_fuzz(l1,dist1,word):\n",
    "    for i in l1:\n",
    "        distant = SequenceMatcher(None,i, word)\n",
    "        rate=distant.ratio()\n",
    "        if rate<=dist1:\n",
    "            dist1=rate\n",
    "        else:\n",
    "            continue\n",
    "    print([x for x in l1 if SequenceMatcher(None,x, word).ratio()==dist1])\n",
    "    print(distant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "m = SequenceMatcher(None,\"NEW YORK METS\", \"NEW YORK MEATS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9629629629629629"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'if', 'in', 'and', 'about', 'all', 'big', 'to', 'a', 'talk', 'will']\n",
      "<difflib.SequenceMatcher object at 0x000000E3C5F672B0>\n"
     ]
    }
   ],
   "source": [
    "min_dist_fuzz(tok,dist1,word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok= set(tok) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist1=0\n",
    "word='cherry'\n",
    "def min_dist_fuzz_spar(l1,dist1,word):\n",
    "    for i in l1:\n",
    "        distant = fuzz.partial_ratio(i, word)\n",
    "        if distant>=dist1:\n",
    "            dist1=distant       \n",
    "        else:\n",
    "            continue\n",
    "    print([x for x in l1 if fuzz.partial_ratio(x, word)==dist1])\n",
    "    #print(dist1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the']\n"
     ]
    }
   ],
   "source": [
    "min_dist_fuzz_spar(tok,dist1,word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fuzzywuzzyNote: you may need to restart the kernel to use updated packages.\n",
      "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Installing collected packages: fuzzywuzzy\n",
      "\n",
      "Successfully installed fuzzywuzzy-0.18.0\n"
     ]
    }
   ],
   "source": [
    "pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'vocabulary']\n"
     ]
    }
   ],
   "source": [
    "min_dist_fuzz_spar(tok,dist1,'vocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['understand']\n"
     ]
    }
   ],
   "source": [
    "min_dist_fuzz_spar(tok,dist1,'under')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['big', 'to']\n"
     ]
    }
   ],
   "source": [
    "min_dist_fuzz_spar(tok,dist1,'sight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['to', 'a']\n"
     ]
    }
   ],
   "source": [
    "min_dist_fuzz_spar(tok,dist1,'towards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a']\n"
     ]
    }
   ],
   "source": [
    "min_dist_fuzz_spar(tok,dist1,'blame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the']\n"
     ]
    }
   ],
   "source": [
    "min_dist_fuzz_spar(tok,dist1,'cherry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'if', 'in', 'and', 'about', 'all', 'big', 'to', 'a', 'talk', 'will']\n",
      "<difflib.SequenceMatcher object at 0x000000E3CA87C2E0>\n"
     ]
    }
   ],
   "source": [
    "min_dist_fuzz(tok,dist1,'merry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'if', 'in', 'about', 'all', 'big', 'to', 'a', 'you', 'talk', 'will']\n",
      "<difflib.SequenceMatcher object at 0x000000E3CA87C220>\n"
     ]
    }
   ],
   "source": [
    "min_dist_fuzz(tok,dist1,'cheered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'if', 'in', 'big', 'be', 'give', 'never', 'will', 'less']\n",
      "<difflib.SequenceMatcher object at 0x000000E3CA87C9D0>\n"
     ]
    }
   ],
   "source": [
    "min_dist_fuzz(tok,dist1,'today') #Sparse is necessary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
